#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Aè‚¡çŸ­çº¿äº¤æ˜“ç³»ç»Ÿ - çœŸå®žæ•°æ®è¿è¡Œç‰ˆæœ¬
æ”¯æŒä»ŽCSVåŠ è½½çœŸå®žè¡Œæƒ…æ•°æ®ï¼ˆç”¨æˆ·å¯ä»Žé€šè¾¾ä¿¡ã€åŒèŠ±é¡ºå¯¼å‡ºï¼‰
"""
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd
import numpy as np
from datetime import datetime, timedelta

print("=" * 70)
print("ðŸŽ¬ Aè‚¡çŸ­çº¿äº¤æ˜“ç³»ç»Ÿ - çœŸå®žæ•°æ®ç‰ˆæœ¬")
print("=" * 70)

# ========== æ£€æŸ¥çœŸå®žæ•°æ®æ–‡ä»¶ ==========
print("\n[æ£€æŸ¥] å¯»æ‰¾çœŸå®žæ•°æ®æº...")

csv_files = [f for f in os.listdir("data") if f.endswith(".csv")] if os.path.exists("data") else []

if csv_files:
    print(f"  âœ“ æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")
    print(f"    {', '.join(csv_files)}")
else:
    print("\n  âš ï¸  æœªæ‰¾åˆ°çœŸå®žæ•°æ®æ–‡ä»¶ï¼")
    print("\n  ðŸ’¡ èŽ·å–çœŸå®žæ•°æ®çš„æ–¹å¼ï¼š")
    print("     1ï¸âƒ£  ä»ŽåŒèŠ±é¡º/é€šè¾¾ä¿¡/ä¸œæ–¹è´¢å¯Œå¯¼å‡ºCSV")
    print("     2ï¸âƒ£  æ‰‹å·¥æ”¾å…¥ data/ ç›®å½•")
    print("     3ï¸âƒ£  æˆ–è€…ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•èŽ·å–ï¼š")
    print()
    print("     æ–¹æ¡ˆA: ä½¿ç”¨ tushare (éœ€ä»˜è´¹ï¼ŒæŽ¨è)")
    print("       pip install tushare")
    print("       # ç„¶åŽä¿®æ”¹è„šæœ¬ä¸‹è½½æ•°æ®")
    print()
    print("     æ–¹æ¡ˆB: ä½¿ç”¨ yfinance (å…è´¹ï¼Œå›½é™…æ•°æ®)")
    print("       pip install yfinance")
    print()
    print("     æ–¹æ¡ˆC: æ‰‹å·¥ä¸‹è½½åŽæ”¾å…¥ data/ ç›®å½•")
    print("       æ ¼å¼: code,date,open,high,low,close,volume")
    print()
    
    # åˆ›å»ºç¤ºä¾‹CSV
    print("  ðŸ”§ åˆ›å»ºç¤ºä¾‹çœŸå®žæ•°æ®ç»“æž„...")
    
    sample_data = {
        "date": pd.date_range("2023-01-01", periods=30, freq="D"),
        "code": ["000001"] * 30,
        "open": 100 + np.random.randn(30).cumsum() * 0.5,
        "high": 102 + np.random.randn(30).cumsum() * 0.5,
        "low": 98 + np.random.randn(30).cumsum() * 0.5,
        "close": 100 + np.random.randn(30).cumsum() * 0.5,
        "volume": np.random.randint(10000000, 100000000, 30),
    }
    
    sample_df = pd.DataFrame(sample_data)
    os.makedirs("data", exist_ok=True)
    sample_df.to_csv("data/sample_000001.csv", index=False)
    print("  âœ“ å·²åˆ›å»ºç¤ºä¾‹æ–‡ä»¶: data/sample_000001.csv")
    print("  âœ“ è¯·å‚è€ƒæ­¤æ ¼å¼æ·»åŠ çœŸå®žæ•°æ®")
    sys.exit(1)

# ========== åŠ è½½çœŸå®žæ•°æ® ==========
print("\n[ç¬¬1æ­¥] åŠ è½½çœŸå®žè¡Œæƒ…æ•°æ®...")

all_data = []
for csv_file in csv_files:
    try:
        df = pd.read_csv(f"data/{csv_file}")
        all_data.append(df)
        print(f"  âœ“ å·²åŠ è½½ {csv_file}: {len(df)} è¡Œ")
    except Exception as e:
        print(f"  âœ— åŠ è½½ {csv_file} å¤±è´¥: {e}")

if not all_data:
    print("  âœ— æ— æ³•åŠ è½½ä»»ä½•æ•°æ®")
    sys.exit(1)

# åˆå¹¶æ•°æ®
data = pd.concat(all_data, ignore_index=True)
data["date"] = pd.to_datetime(data["date"])
data = data.sort_values(["code", "date"]).reset_index(drop=True)

print(f"\n  ðŸ“Š æ•°æ®æ¦‚è§ˆ:")
print(f"    - è‚¡ç¥¨æ•°: {data['code'].nunique()}")
print(f"    - äº¤æ˜“æ—¥: {data['date'].min().date()} ~ {data['date'].max().date()}")
print(f"    - æ€»è®°å½•: {len(data)}")

# ========== æž„å»ºé¢æ¿æ•°æ® ==========
print("\n[ç¬¬2æ­¥] æž„å»ºé¢æ¿æ•°æ®...")

closes_list = []
volumes_list = []

for code in data["code"].unique():
    code_data = data[data["code"] == code].set_index("date")[["close", "volume"]]
    closes_list.append(code_data[["close"]].rename(columns={"close": code}))
    volumes_list.append(code_data[["volume"]].rename(columns={"volume": code}))

closes = pd.concat(closes_list, axis=1).dropna(how="all")
volumes = pd.concat(volumes_list, axis=1).dropna(how="all")

print(f"  âœ“ é¢æ¿æ•°æ®: {closes.shape[0]} äº¤æ˜“æ—¥ Ã— {closes.shape[1]} åªè‚¡ç¥¨")

# ========== ç¬¬3æ­¥ï¼šè®¡ç®—å¸‚åœºå®½åº¦ ==========
print("\n[ç¬¬3æ­¥] è®¡ç®—å¸‚åœºå®½åº¦ä¸Žæƒ…ç»ª...")

pct_change = closes.pct_change() * 100
up_count = (pct_change > 0).sum(axis=1)
down_count = (pct_change < 0).sum(axis=1)
total = (pct_change.notna()).sum(axis=1)

ad_ratio = (up_count - down_count) / total

# 52å‘¨é«˜ä½Ž
roll_max = closes.rolling(252, min_periods=20).max()
roll_min = closes.rolling(252, min_periods=20).min()
nh_252 = (closes >= (roll_max * 0.999)).sum(axis=1)
nl_252 = (closes <= (roll_min * 1.001)).sum(axis=1)

nh_ratio = nh_252 / total.clip(lower=1)
nl_ratio = nl_252 / total.clip(lower=1)

# Z-score å¾—åˆ†
def zscore(s):
    m = s.rolling(min(252, len(s)//2), min_periods=5).mean()
    sd = s.rolling(min(252, len(s)//2), min_periods=5).std().replace(0, np.nan)
    return (s - m) / sd

breadth_score = 0.5 * zscore(ad_ratio).clip(-3,3) + 0.3 * zscore(nh_ratio - nl_ratio).clip(-3,3)
breadth_score_ema = breadth_score.ewm(span=5, adjust=False, min_periods=5).mean()

# å¸‚åœºæ€åŠ¿
regime = pd.Series(index=breadth_score_ema.index, dtype="object")
regime[breadth_score_ema > 0.5] = "Bull"
regime[breadth_score_ema < -0.5] = "Bear"
regime[(breadth_score_ema <= 0.5) & (breadth_score_ema >= -0.5)] = "Neutral"

breadth = pd.DataFrame({
    "up_count": up_count,
    "down_count": down_count,
    "ad_ratio": ad_ratio,
    "nh_ratio": nh_ratio,
    "breadth_score_ema": breadth_score_ema,
    "regime": regime,
}).dropna()

print(f"  âœ“ å®½åº¦è®¡ç®—å®Œæˆ: {len(breadth)} æ—¥")
print(f"  âœ“ æœ€è¿‘æ€åŠ¿: {breadth['regime'].iloc[-1]} (å¾—åˆ†: {breadth['breadth_score_ema'].iloc[-1]:.2f})")

os.makedirs("data/market", exist_ok=True)
breadth.to_parquet("data/market/breadth_real.parquet")
print(f"  âœ“ å·²ä¿å­˜åˆ° data/market/breadth_real.parquet")

# ========== ç¬¬4æ­¥ï¼šç”Ÿæˆé€‰è‚¡ä¿¡å· ==========
print("\n[ç¬¬4æ­¥] ç”Ÿæˆé€‰è‚¡ä¿¡å·...")

ma5 = closes.rolling(5).mean()
ma20 = closes.rolling(20).mean()
mom5 = closes.pct_change(5)
vol20 = closes.pct_change().rolling(20).std()
vr = volumes / volumes.rolling(20).mean()

# æŽ’åºæ‰“åˆ†
q_mom = mom5.rank(axis=1, pct=True)
q_vol = vol20.rank(axis=1, pct=True)
score = q_mom - q_vol

# å…¥åœºä¸Žå‡ºåœºä¿¡å·
entries = (ma5 > ma20) & (q_mom > 0.6) & (q_vol < 0.8) & (vr > 1)
exits = ma5 < ma20

print(f"  âœ“ å…¥åœºä¿¡å·: {entries.sum().sum():.0f} ä¸ª")
print(f"  âœ“ å‡ºåœºä¿¡å·: {exits.sum().sum():.0f} ä¸ª")

# ========== ç¬¬5æ­¥ï¼šé£ŽæŽ§è¿‡æ»¤ ==========
print("\n[ç¬¬5æ­¥] åº”ç”¨é£ŽæŽ§è¿‡æ»¤...")

reg = breadth["regime"].reindex(entries.index).ffill()
entries_filtered = entries & (reg != "Bear")

mask_top = score.apply(lambda s: s.rank(ascending=False) <= 20, axis=1)
entries_top = entries_filtered & mask_top

print(f"  âœ“ è¿‡æ»¤åŽå…¥åœºä¿¡å·: {entries_top.sum().sum():.0f} ä¸ª")

# ä»“ä½ç¼©æ”¾
sig = 1 / (1 + np.exp(-breadth["breadth_score_ema"]))
position_scale = (0.2 + sig).clip(0, 1)

print(f"  âœ“ ä»“ä½ç¼©æ”¾èŒƒå›´: {position_scale.min():.2%} ~ {position_scale.max():.2%}")

# ========== ç¬¬6æ­¥ï¼šå¯¼å‡ºè®¢å• ==========
print("\n[ç¬¬6æ­¥] ç”Ÿæˆå¹¶å¯¼å‡ºè®¢å•...")

today = entries_top.index[-1]
picks_mask = entries_top.loc[today].fillna(False)
picks = picks_mask[picks_mask].index.tolist()

if len(picks) == 0:
    print(f"  âš ï¸  ä»Šæ—¥æ— å…¥åœºä¿¡å· (å¸‚åœºæ€åŠ¿: {breadth['regime'].iloc[-1]})")
else:
    picks = score.loc[today][picks].nlargest(20).index.tolist()
    n_picks = len(picks)
    alloc = round(1 / n_picks, 4)
    
    orders_df = pd.DataFrame({
        "code": picks,
        "target_weight": alloc,
        "order_type": "buy",
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
    })
    
    orders_df.to_csv("data/orders_real.csv", index=False, encoding="utf-8-sig")
    
    print(f"  âœ“ å·²å¯¼å‡º {n_picks} ä¸ªä¿¡å·åˆ° data/orders_real.csv")
    print(f"  âœ“ æ¯ä¸ªæ ‡çš„æƒé‡: {alloc:.2%}")
    print(f"\n  ðŸ“‹ è®¢å•é¢„è§ˆ (å‰5ä¸ª):")
    print(orders_df.head(5).to_string(index=False))

# ========== ç”ŸæˆæŠ¥å‘Š ==========
print("\n" + "=" * 70)
print("ðŸ“Š çœŸå®žæ•°æ®è¿è¡Œå®Œæˆ")
print("=" * 70)

print(f"\nðŸ“ˆ å¸‚åœºçŠ¶æ€:")
print(f"  - äº¤æ˜“æ—¥: {today.strftime('%Y-%m-%d')}")
print(f"  - å¸‚åœºæ€åŠ¿: {breadth['regime'].iloc[-1]}")
print(f"  - å®½åº¦å¾—åˆ†: {breadth['breadth_score_ema'].iloc[-1]:.2f}")
print(f"  - ä¸Šæ¶¨: {int(breadth['up_count'].iloc[-1])}, ä¸‹è·Œ: {int(breadth['down_count'].iloc[-1])}")

print(f"\nðŸ’° ä»“ä½ç®¡ç†:")
print(f"  - ä»Šæ—¥ç¼©æ”¾: {position_scale.iloc[-1]:.2%}")

print("\nâœ… çœŸå®žæ•°æ®å¤„ç†å®Œæˆï¼")
print("=" * 70)
